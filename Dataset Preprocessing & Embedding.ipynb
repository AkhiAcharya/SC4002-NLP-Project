{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d8c358-fd02-4d7b-a9db-6d5bb9bf4188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.0.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69fbc661-1c81-42e9-8a6d-c56d5a8aad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adithyasamudrala/SC4002-NLP-Project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f157e3-981a-4787-8322-5a2ead0804c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ade63b-ca74-4973-af2f-b356aef66c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  label\n",
      "0   lovingly photographed in the manner of a golde...      1\n",
      "1               consistently clever and suspenseful .      1\n",
      "2   it's like a \" big chill \" reunion of the baade...      1\n",
      "3   the story gives ample opportunity for large-sc...      1\n",
      "4                   red dragon \" never cuts corners .      1\n",
      "5   fresnadillo has something serious to say about...      1\n",
      "6   throws in enough clever and unexpected twists ...      1\n",
      "7   weighty and ponderous but every bit as filling...      1\n",
      "8   a real audience-pleaser that will strike a cho...      1\n",
      "9   generates an enormous feeling of empathy for i...      1\n",
      "10  exposing the ways we fool ourselves is one hou...      1\n",
      "11  it's up to you to decide whether to admire the...      1\n",
      "12  mostly , [goldbacher] just lets her complicate...      1\n",
      "13  . . . quite good at providing some good old fa...      1\n",
      "14  at its worst , the movie is pretty diverting ;...      1\n",
      "{'text': ['lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .', 'consistently clever and suspenseful .', 'it\\'s like a \" big chill \" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .', 'the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .', 'red dragon \" never cuts corners .', 'fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .', 'throws in enough clever and unexpected twists to make the formula feel fresh .', 'weighty and ponderous but every bit as filling as the treat of the title .', \"a real audience-pleaser that will strike a chord with anyone who's ever waited in a doctor's office , emergency room , hospital bed or insurance company office .\", 'generates an enormous feeling of empathy for its characters .', \"exposing the ways we fool ourselves is one hour photo's real strength .\", \"it's up to you to decide whether to admire these people's dedication to their cause or be repelled by their dogmatism , manipulativeness and narrow , fearful view of american life .\", 'mostly , [goldbacher] just lets her complicated characters be unruly , confusing and , through it all , human .', '. . . quite good at providing some good old fashioned spooks .', 'at its worst , the movie is pretty diverting ; the pity is that it rarely achieves its best .'], 'label': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_dataset.to_pandas().head(15))\n",
    "\n",
    "print(test_dataset[:15])  # View the first 5 rows as a dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc6a14-2a1b-4c18-a271-047bd4414647",
   "metadata": {},
   "source": [
    "# Google Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67690c1a-a274-4abf-b6fb-7d628b61d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # Path to the .bin file\n",
    "# model_path = 'GoogleNews-vectors-negative300.bin'  # Update with the exact path to the .bin file\n",
    "\n",
    "# # Load the Word2Vec model\n",
    "# word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "# print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff10a49d-ad95-444e-842e-3d4c952b6904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded and saved at: /Users/adithyasamudrala/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Define the model path\n",
    "model_name = \"word2vec-google-news-300\"\n",
    "\n",
    "path = api.load(model_name, return_path=True)\n",
    "print(f\"Model downloaded and saved at: {path}\")\n",
    "\n",
    "\n",
    "# Load the model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b5241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677787bc-b346-4868-b40f-e080afb3bb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'king':\n",
      "kings: 0.7138\n",
      "queen: 0.6511\n",
      "monarch: 0.6413\n",
      "crown_prince: 0.6204\n",
      "prince: 0.6160\n",
      "sultan: 0.5865\n",
      "ruler: 0.5798\n",
      "princes: 0.5647\n",
      "Prince_Paras: 0.5433\n",
      "throne: 0.5422\n"
     ]
    }
   ],
   "source": [
    "# Find words similar to \"king\"\n",
    "similar_words = word2vec_model.most_similar(\"king\", topn=10)\n",
    "print(\"Words similar to 'king':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872da37-9aaa-4cde-a2ff-bd55cd12e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity between \"king\" and \"queen\"\n",
    "similarity = word2vec_model.similarity(\"king\", \"queen\")\n",
    "print(f\"Similarity between 'king' and 'queen': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea02728-46a8-4830-a059-59f7cd5817ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'apple': [ 0.12597656  0.19042969  0.06982422  0.07226562 -0.21875     0.02758789\n",
      "  0.05395508 -0.07666016  0.04296875 -0.12988281  0.05151367 -0.10498047\n",
      " -0.11474609  0.05566406 -0.05029297 -0.00424194  0.08447266  0.00915527\n",
      " -0.09277344 -0.02770996  0.08935547 -0.11425781 -0.13476562 -0.09521484\n",
      " -0.10742188 -0.10888672 -0.02563477  0.31054688  0.17382812  0.19921875\n",
      "  0.0090332   0.02905273 -0.09863281 -0.10058594 -0.21191406 -0.11962891\n",
      " -0.06542969  0.125      -0.07617188 -0.11181641 -0.19726562  0.12695312\n",
      "  0.02770996  0.04785156  0.11035156  0.04760742  0.01330566  0.19726562\n",
      "  0.11962891  0.04296875  0.07861328 -0.07128906  0.21972656  0.00772095\n",
      " -0.16210938  0.125       0.10498047 -0.03588867 -0.03710938 -0.11767578\n",
      " -0.29296875  0.11669922  0.04077148 -0.00152588 -0.06445312  0.24023438\n",
      "  0.29101562 -0.03198242 -0.02160645  0.08398438  0.0703125   0.05175781\n",
      "  0.17871094 -0.05639648  0.09472656 -0.01275635  0.02050781  0.13867188\n",
      " -0.00069427  0.26757812  0.17480469 -0.13574219  0.05517578  0.14941406\n",
      "  0.05395508 -0.01611328 -0.05712891  0.23046875  0.09033203 -0.05078125\n",
      " -0.0324707  -0.02038574 -0.16113281 -0.23144531  0.08105469 -0.01318359\n",
      " -0.07617188  0.09326172 -0.22753906  0.05834961 -0.07763672 -0.19726562\n",
      " -0.18164062  0.07421875 -0.06396484  0.15136719 -0.00349426 -0.13964844\n",
      " -0.09423828 -0.29492188  0.09716797  0.0168457   0.00457764  0.05224609\n",
      "  0.140625    0.109375   -0.07714844  0.11279297  0.23632812  0.08447266\n",
      " -0.11230469 -0.13671875 -0.22167969  0.06689453 -0.04516602  0.28515625\n",
      "  0.07470703 -0.00524902  0.17578125  0.1484375  -0.12890625  0.18652344\n",
      "  0.01098633  0.01098633  0.12988281 -0.01940918  0.12597656 -0.05444336\n",
      " -0.05322266  0.16015625 -0.11865234 -0.0112915   0.10888672 -0.02575684\n",
      "  0.00836182  0.00215149 -0.06005859  0.09228516  0.03710938  0.15136719\n",
      "  0.20800781 -0.12060547 -0.04711914  0.12792969  0.09521484  0.2421875\n",
      "  0.02087402 -0.29492188 -0.10058594 -0.15917969 -0.06884766  0.1328125\n",
      " -0.11572266  0.25585938 -0.00245667 -0.02233887  0.14257812 -0.1796875\n",
      "  0.00230408  0.22265625 -0.01434326 -0.02783203  0.14648438 -0.2734375\n",
      "  0.09667969 -0.13183594  0.07080078 -0.04370117 -0.17089844  0.12158203\n",
      " -0.01495361 -0.05395508 -0.08789062 -0.01330566 -0.13378906  0.25390625\n",
      "  0.04663086 -0.02380371  0.19824219 -0.09960938 -0.04833984  0.11132812\n",
      " -0.02893066  0.14648438 -0.14550781  0.0546875  -0.11035156 -0.01165771\n",
      " -0.25585938 -0.08642578 -0.14453125  0.08789062 -0.01611328  0.19433594\n",
      "  0.09277344  0.09716797  0.03491211  0.15136719  0.12109375 -0.10253906\n",
      "  0.05175781  0.16796875  0.23535156 -0.0213623  -0.2578125  -0.04370117\n",
      "  0.10693359 -0.00540161 -0.12597656  0.15332031  0.15527344 -0.14453125\n",
      " -0.10302734 -0.1328125   0.20996094  0.24023438 -0.00112915 -0.15039062\n",
      "  0.15136719  0.16308594 -0.20117188 -0.24902344  0.25585938 -0.01013184\n",
      " -0.0625     -0.21679688  0.015625   -0.01647949  0.13867188 -0.21875\n",
      "  0.09521484 -0.15234375 -0.02502441 -0.00457764  0.01037598  0.09326172\n",
      "  0.20507812  0.03491211  0.06738281 -0.14453125  0.15917969 -0.03393555\n",
      "  0.02563477  0.05371094  0.1640625   0.12109375  0.18945312 -0.03735352\n",
      "  0.09423828  0.05566406  0.21777344 -0.00466919 -0.12255859 -0.06176758\n",
      " -0.07958984  0.20800781  0.20214844 -0.00582886 -0.10986328  0.19335938\n",
      "  0.07910156  0.06933594  0.09619141 -0.06542969  0.11816406  0.0246582\n",
      " -0.09521484 -0.00367737 -0.09570312  0.12207031 -0.05859375  0.18847656\n",
      "  0.06982422  0.07568359  0.0324707  -0.00909424 -0.04589844 -0.10253906\n",
      " -0.20996094 -0.18652344  0.0703125   0.01843262  0.04736328  0.21191406\n",
      "  0.10595703 -0.06591797  0.01940918  0.0612793   0.17285156 -0.07861328]\n"
     ]
    }
   ],
   "source": [
    "# Get the vector for the word \"apple\"\n",
    "vector = word2vec_model[\"an\"]\n",
    "print(\"Vector for 'apple':\", vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdba58f-ad61-4fbe-b87d-35f1db47d633",
   "metadata": {},
   "source": [
    "# GLOVE Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13fd9a36-85fb-4cd1-afcc-22a45eb02aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================----------------------------] 45.1% 169.5/376.1MB downloaded"
     ]
    }
   ],
   "source": [
    "glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "print(\"GloVe model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dadfe6d-6282-41e2-ab17-80c72cd51661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Testing the GloVe model\n",
    "\n",
    "# 1. Check similar words\n",
    "print(\"Words similar to 'king':\")\n",
    "similar_words = glove_model.most_similar(\"king\", topn=10)\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n",
    "\n",
    "# 2. Compute word similarity\n",
    "similarity = glove_model.similarity(\"king\", \"queen\")\n",
    "print(f\"\\nSimilarity between 'king' and 'queen': {similarity:.4f}\")\n",
    "\n",
    "# 3. Perform analogy\n",
    "analogy_result = glove_model.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=1)\n",
    "print(\"\\nAnalogy result for 'king - man + woman':\", analogy_result)\n",
    "\n",
    "# 4. Check for a word in the vocabulary\n",
    "word = \"apple\"\n",
    "if word in glove_model:\n",
    "    print(f\"\\n'{word}' is in the vocabulary.\")\n",
    "else:\n",
    "    print(f\"\\n'{word}' is not in the vocabulary.\")\n",
    "\n",
    "# 5. Get word vector\n",
    "vector = glove_model[\"apple\"]\n",
    "print(\"\\nVector for 'apple':\", vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ac832-93ec-452b-a5f7-d25801f47d23",
   "metadata": {},
   "source": [
    "# Q1a) What is the size of the vocabulary formed from your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f90ada-c902-439d-b1b0-c79a74f509e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    # Use simple whitespace tokenization and lowercasing\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Count unique tokens\n",
    "counter = Counter()\n",
    "for example in train_dataset:\n",
    "    tokens = tokenize(example[\"text\"])\n",
    "    counter.update(tokens)\n",
    "\n",
    "# Size of the vocabulary\n",
    "vocab_size = len(counter)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d39538-b899-4ca4-86b1-2c5d33444988",
   "metadata": {},
   "source": [
    "# Q1b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde6b4c-4cd3-4754-bc23-b847b1b71d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for example in train_dataset:\n",
    "    tokens = tokenize(example[\"text\"])\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    for token in tokens:\n",
    "        if token not in glove_model:\n",
    "            counter += 1\n",
    "            print(token)\n",
    "\n",
    "print(\"Number of OOV words:\", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb29735-f027-496a-9475-796ea06d9a40",
   "metadata": {},
   "source": [
    "# Q1c) The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove). Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you think is the best strategy to mitigate such limitation? Implement your solution in your source code. Show the corresponding code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba73f6-d5fd-4870-b8af-41710e463d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
