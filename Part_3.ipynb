{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PSp4YNgfAzh"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGaQi07An4Sy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/SC4002 Natural Language Processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddJY5P-EcRym"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/drive/MyDrive/SC4002 Natural Language Processing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ms8MX9MWegm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EPSZYlQiCi2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/SC4002-NLP-Project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whWbWXSSYalQ"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"rotten_tomatoes\")\n",
        "train_dataset = dataset['train']\n",
        "validation_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM0OKskx4IH1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def save_model_to_drive(model_name):\n",
        "    import gensim.downloader as api\n",
        "    from gensim.models import KeyedVectors\n",
        "    import os\n",
        "\n",
        "    save_path = '/content/drive/MyDrive/models'\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    if model_name == 'word2vec':\n",
        "        path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
        "        model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
        "        model.save(f'{save_path}/word2vec.model')\n",
        "        print(\"Word2Vec model saved to Google Drive successfully!\")\n",
        "\n",
        "    elif model_name == 'glove':\n",
        "        model = api.load(\"glove-wiki-gigaword-300\")\n",
        "        model.save(f'{save_path}/glove.model')\n",
        "        print(\"GloVe model saved to Google Drive successfully!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_model_from_drive(model_name):\n",
        "    from gensim.models import KeyedVectors\n",
        "    import os\n",
        "\n",
        "    model_path = f'/content/drive/MyDrive/models/{model_name}.model'\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        model = KeyedVectors.load(model_path)\n",
        "        print(f\"{model_name.capitalize()} model loaded from Drive successfully!\")\n",
        "        return model\n",
        "    else:\n",
        "        print(f\"{model_name.capitalize()} model not found in Drive. Downloading and saving...\")\n",
        "        return save_model_to_drive(model_name)\n",
        "\n",
        "def get_model(model_name):\n",
        "    if model_name not in ['word2vec', 'glove']:\n",
        "        raise ValueError(\"model_name must be either 'word2vec' or 'glove'\")\n",
        "\n",
        "    try:\n",
        "        return load_model_from_drive(model_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {model_name} model: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "word2vec_model = get_model('word2vec')\n",
        "\n",
        "glove_model = get_model('glove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT2Dpqj-rqqx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gensim.downloader as api\n",
        "import pickle\n",
        "\n",
        "def load_and_save_fasttext_model(drive_path=\"/content/drive/MyDrive/models/fasttext_model.model\"):\n",
        "    \"\"\"\n",
        "    Load the FastText model for OOV handling. Save the model to Google Drive if it does not exist.\n",
        "\n",
        "    Args:\n",
        "        drive_path (str): Path in Google Drive to save/load the FastText model.\n",
        "\n",
        "    Returns:\n",
        "        model: Loaded FastText model.\n",
        "    \"\"\"\n",
        "    # Check if the model already exists in Google Drive\n",
        "    if os.path.exists(drive_path):\n",
        "        print(\"Loading FastText model from Drive...\")\n",
        "        fasttext_model = api.load(drive_path)\n",
        "        print(\"FastText model loaded from Drive successfully.\")\n",
        "    else:\n",
        "        print(\"Downloading FastText model...\")\n",
        "        # Load FastText model from gensim\n",
        "        fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "        print(\"FastText model loaded successfully.\")\n",
        "\n",
        "    return fasttext_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs3jGkJDZ-09"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Load FastText model for OOV handling\n",
        "fasttext_model = load_and_save_fasttext_model()\n",
        "print(\"FastText model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jqx243DGoUX"
      },
      "outputs": [],
      "source": [
        "from utils import get_embedding, get_improved_embedding\n",
        "\n",
        "word_to_vector_map = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yir0Qy9G6pg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDGC-110uTK_"
      },
      "outputs": [],
      "source": [
        "from utils import SentimentDataset\n",
        "from utils import get_device\n",
        "\n",
        "device = get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npmynSczkpA7"
      },
      "outputs": [],
      "source": [
        "print(type(train_dataset))\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKjVebv9lYEy"
      },
      "outputs": [],
      "source": [
        "def prepare_data(dataset):\n",
        "    texts = [example['text'] for example in dataset]\n",
        "    labels = [example['label'] for example in dataset]\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = prepare_data(train_dataset)\n",
        "val_texts, val_labels = prepare_data(validation_dataset)\n",
        "test_texts, test_labels = prepare_data(test_dataset)\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "train_dataset = SentimentDataset(\n",
        "    train_texts, train_labels, glove_model, word_to_vector_map, get_improved_embedding, fasttext_model\n",
        ")\n",
        "\n",
        "val_dataset = SentimentDataset(\n",
        "    val_texts, val_labels, glove_model, word_to_vector_map, get_improved_embedding, fasttext_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4m3s_-LleEk"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq6vn6RHeQ2E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmxKdOXdz49B"
      },
      "source": [
        "# **BiLSTMModel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQNCdhAERFy1"
      },
      "outputs": [],
      "source": [
        "from BiLSTM import BiLSTMModel\n",
        "from utils import train_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_DZTpUpV4XS"
      },
      "outputs": [],
      "source": [
        "bilstm = BiLSTMModel(**BiLSTMModel.config).to(device)\n",
        "bilstm_state,_,_ = train_model(\n",
        "    bilstm,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=30,\n",
        "    device=device,\n",
        "    model_name='BiLSTM'\n",
        ")\n",
        "bilstm.load_state_dict(bilstm_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qklL3SxKzqc6"
      },
      "source": [
        "# **BiGRUMODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6W55-P-Z-0-"
      },
      "outputs": [],
      "source": [
        "from BiGRU import BiGRUModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdc9-gW9mAY9"
      },
      "outputs": [],
      "source": [
        "\n",
        "bigru = BiGRUModel(**BiGRUModel.config).to(device)\n",
        "bigru_state,_,_ = train_model(\n",
        "    bigru,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=30,\n",
        "    device=device,\n",
        "    model_name='BiGRU'\n",
        ")\n",
        "bigru.load_state_dict(bigru_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9aq-DwA0Mjw"
      },
      "source": [
        "# **CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbfbAg4GZ-0_"
      },
      "outputs": [],
      "source": [
        "from CNN import CNNModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ78faftmMtc"
      },
      "outputs": [],
      "source": [
        "\n",
        "cnn = CNNModel(**CNNModel.config).to(device)\n",
        "\n",
        "cnn_state,_,_ = train_model(\n",
        "    cnn,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=1,\n",
        "    device=device,\n",
        "    model_name='CNN'\n",
        ")\n",
        "cnn.load_state_dict(cnn_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ztVhbabWT6B"
      },
      "source": [
        "# Enhanced Bi-LSTM using Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2324eiuZ-0_"
      },
      "outputs": [],
      "source": [
        "from AttentiveBiLSTM import AttentiveBiLSTMModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z8awOXyWcj7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "attentive_bilstm = AttentiveBiLSTMModel(**AttentiveBiLSTMModel.config).to(device)\n",
        "attentive_bilstm_state,_,_ = train_model(\n",
        "    attentive_bilstm,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=1,\n",
        "    device=device,\n",
        "    model_name='AttentiveBiLSTM'\n",
        ")\n",
        "attentive_bilstm_state.load_state_dict(attentive_bilstm_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQG3VzEE0UzS"
      },
      "source": [
        "# **RESULTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRfWdNwGe3f9"
      },
      "outputs": [],
      "source": [
        "test_dataset = SentimentDataset(\n",
        "    test_texts, test_labels, glove_model, word_to_vector_map, get_embedding, fasttext_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa_Jd5wZnfQF"
      },
      "outputs": [],
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgUEVCmur7AJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K64zVak7vQFq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    confusion_matrix, roc_curve, auc, mean_squared_error\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model_comprehensive(model, test_loader, criterion, device, model_name, glove_model, fasttext_model, word_to_vector_map):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    total_loss = 0\n",
        "\n",
        "    print(f\"Starting evaluation of {model_name}\")\n",
        "    print(f\"Number of batches in test loader: {len(test_loader)}\")\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (batch_words, labels) in enumerate(test_loader):\n",
        "\n",
        "\n",
        "            batch_size = len(batch_words)\n",
        "            max_length = len(batch_words[0])\n",
        "            processed_inputs = torch.zeros((batch_size, max_length, 300), device=device)\n",
        "\n",
        "            # Convert words to embeddings\n",
        "            for i in range(batch_size):\n",
        "                for j in range(max_length):\n",
        "                    word = batch_words[i][j]\n",
        "                    embedding_models = {\n",
        "                        \"glove\": glove_model,\n",
        "                        \"fasttext\": fasttext_model\n",
        "                    }\n",
        "                    embedding = word\n",
        "                    processed_inputs[i][j] = torch.tensor(embedding, device=device)\n",
        "            labels = labels.to(device)\n",
        "            if len(labels.shape) > 1:\n",
        "                labels = labels.squeeze()\n",
        "\n",
        "\n",
        "            outputs = model(processed_inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs[:,1].cpu().numpy())\n",
        "\n",
        "    print(\"\\nEvaluation completed successfully\")\n",
        "    print(f\"Total samples processed: {len(all_predictions)}\")\n",
        "\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
        "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(all_labels, all_predictions))\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "\n",
        "    print(f\"\\nTest Results for {model_name}\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'rmse': rmse,\n",
        "        'roc_auc': roc_auc,\n",
        "        'conf_matrix': conf_matrix,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'predictions': all_predictions,\n",
        "        'labels': all_labels,\n",
        "        'probabilities': all_probs\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, model_name):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curves(results_dict):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for result in results_dict.values():\n",
        "        plt.plot(\n",
        "            result['fpr'],\n",
        "            result['tpr'],\n",
        "            label=f\"{result['model_name']} (AUC = {result['roc_auc']:.3f})\"\n",
        "        )\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves Comparison')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_metrics_comparison(results_dict):\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'rmse', 'roc_auc']\n",
        "    model_names = list(results_dict.keys())\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for idx, metric in enumerate(metrics):\n",
        "        values = [results_dict[model][metric] for model in model_names]\n",
        "        axes[idx].bar(model_names, values)\n",
        "        axes[idx].set_title(metric.upper())\n",
        "        axes[idx].set_ylim(0, max(1.1, max(values) * 1.1))\n",
        "        axes[idx].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "results_dict = {}\n",
        "\n",
        "bilstm.eval()\n",
        "bigru.eval()\n",
        "cnn.eval()\n",
        "attentive_bilstm.eval()\n",
        "\n",
        "models = {\n",
        "    'BiLSTM': bilstm,\n",
        "    'BiGRU': bigru,\n",
        "    'CNN': cnn,\n",
        "    'AttentiveBiLSTM': attentive_bilstm,\n",
        "    'AttentiveBiGRU' : attentive_bigru\n",
        "}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "    results = evaluate_model_comprehensive(\n",
        "        model, test_loader, criterion, device, model_name, glove_model, fasttext_model, word_to_vector_map\n",
        "    )\n",
        "    results_dict[model_name] = results\n",
        "\n",
        "    plot_confusion_matrix(results['conf_matrix'], model_name)\n",
        "\n",
        "plot_roc_curves(results_dict)\n",
        "\n",
        "plot_metrics_comparison(results_dict)\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    'Model': [],\n",
        "    'Loss': [],\n",
        "    'Accuracy': [],\n",
        "    'Precision': [],\n",
        "    'Recall': [],\n",
        "    'F1 Score': [],\n",
        "    'RMSE': [],\n",
        "    'ROC AUC': []\n",
        "})\n",
        "import pandas as pd\n",
        "\n",
        "def create_summary_table(results_dict):\n",
        "    summary_data = []\n",
        "\n",
        "    for model_name, results in results_dict.items():\n",
        "        summary_data.append({\n",
        "            'Model': model_name,\n",
        "            'Loss': results['loss'],\n",
        "            'Accuracy': results['accuracy'],\n",
        "            'Precision': results['precision'],\n",
        "            'Recall': results['recall'],\n",
        "            'F1 Score': results['f1'],\n",
        "            'RMSE': results['rmse'],\n",
        "            'ROC AUC': results['roc_auc']\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "    print(\"\\nSummary of Results:\")\n",
        "    print(\"=\" * 100)\n",
        "    print(summary_df.to_string(index=False, float_format=lambda x: '{:.4f}'.format(x)))\n",
        "\n",
        "    summary_df.to_csv('model_comparison_results.csv', index=False)\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "summary_df = create_summary_table(results_dict)\n",
        "for model_name, results in results_dict.items():\n",
        "    summary_df = create_summary_table(results_dict)\n",
        "\n",
        "print(\"\\nSummary of Results:\")\n",
        "print(\"=\" * 100)\n",
        "print(summary_df.to_string(index=False, float_format=lambda x: '{:.4f}'.format(x)))\n",
        "\n",
        "summary_df.to_csv('model_comparison_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19Qi76TFj21F"
      },
      "outputs": [],
      "source": [
        "print(test_labels)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
