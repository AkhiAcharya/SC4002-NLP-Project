{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2**"
      ],
      "metadata": {
        "id": "edCiP7_-KEho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PSp4YNgfAzh"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGaQi07An4Sy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/SC4002 Natural Language Processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ms8MX9MWegm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhfgP-RBiA7s"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AkhiAcharya/SC4002-NLP-Project.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EPSZYlQiCi2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/SC4002-NLP-Project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whWbWXSSYalQ"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"rotten_tomatoes\")\n",
        "train_dataset = dataset['train']\n",
        "validation_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYJxkCy_Y24b"
      },
      "outputs": [],
      "source": [
        "print(test_dataset.to_pandas().head(15))\n",
        "\n",
        "print(test_dataset[:15])  # View the first 5 rows as a dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM0OKskx4IH1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def save_model_to_drive(model_name):\n",
        "    import gensim.downloader as api\n",
        "    from gensim.models import KeyedVectors\n",
        "    import os\n",
        "\n",
        "    save_path = '/content/drive/MyDrive/models'\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    if model_name == 'word2vec':\n",
        "        path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
        "        model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
        "        model.save(f'{save_path}/word2vec.model')\n",
        "        print(\"Word2Vec model saved to Google Drive successfully!\")\n",
        "\n",
        "    elif model_name == 'glove':\n",
        "        model = api.load(\"glove-wiki-gigaword-300\")\n",
        "        model.save(f'{save_path}/glove.model')\n",
        "        print(\"GloVe model saved to Google Drive successfully!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_model_from_drive(model_name):\n",
        "    from gensim.models import KeyedVectors\n",
        "    import os\n",
        "\n",
        "    model_path = f'/content/drive/MyDrive/models/{model_name}.model'\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        model = KeyedVectors.load(model_path)\n",
        "        print(f\"{model_name.capitalize()} model loaded from Drive successfully!\")\n",
        "        return model\n",
        "    else:\n",
        "        print(f\"{model_name.capitalize()} model not found in Drive. Downloading and saving...\")\n",
        "        return save_model_to_drive(model_name)\n",
        "\n",
        "def get_model(model_name):\n",
        "    if model_name not in ['word2vec', 'glove']:\n",
        "        raise ValueError(\"model_name must be either 'word2vec' or 'glove'\")\n",
        "\n",
        "    try:\n",
        "        return load_model_from_drive(model_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {model_name} model: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "word2vec_model = get_model('word2vec')\n",
        "\n",
        "glove_model = get_model('glove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT2Dpqj-rqqx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gensim.downloader as api\n",
        "import pickle\n",
        "\n",
        "def load_and_save_fasttext_model(drive_path=\"/content/drive/MyDrive/models/fasttext_model.model\"):\n",
        "    \"\"\"\n",
        "    Load the FastText model for OOV handling. Save the model to Google Drive if it does not exist.\n",
        "\n",
        "    Args:\n",
        "        drive_path (str): Path in Google Drive to save/load the FastText model.\n",
        "\n",
        "    Returns:\n",
        "        model: Loaded FastText model.\n",
        "    \"\"\"\n",
        "    # Check if the model already exists in Google Drive\n",
        "    if os.path.exists(drive_path):\n",
        "        print(\"Loading FastText model from Drive...\")\n",
        "        fasttext_model = api.load(drive_path)\n",
        "        print(\"FastText model loaded from Drive successfully.\")\n",
        "    else:\n",
        "        print(\"Downloading FastText model...\")\n",
        "        # Load FastText model from gensim\n",
        "        fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "        print(\"FastText model loaded successfully.\")\n",
        "\n",
        "    return fasttext_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs3jGkJDZ-09"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Load FastText model for OOV handling\n",
        "fasttext_model = load_and_save_fasttext_model()\n",
        "print(\"FastText model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yir0Qy9G6pg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDGC-110uTK_"
      },
      "outputs": [],
      "source": [
        "from utils import SentimentDataset\n",
        "from utils import get_device\n",
        "\n",
        "device = get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npmynSczkpA7"
      },
      "outputs": [],
      "source": [
        "print(type(train_dataset))\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import get_embedding\n",
        "\n",
        "word_to_vector_map = {}\n"
      ],
      "metadata": {
        "id": "q5VFCx9DEJSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKjVebv9lYEy"
      },
      "outputs": [],
      "source": [
        "def prepare_data(dataset):\n",
        "    texts = [example['text'] for example in dataset]\n",
        "    labels = [example['label'] for example in dataset]\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = prepare_data(train_dataset)\n",
        "val_texts, val_labels = prepare_data(validation_dataset)\n",
        "test_texts, test_labels = prepare_data(test_dataset)\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "train_dataset = SentimentDataset(\n",
        "    train_texts, train_labels, glove_model, word_to_vector_map, get_embedding\n",
        ")\n",
        "\n",
        "val_dataset = SentimentDataset(\n",
        "    val_texts, val_labels, glove_model, word_to_vector_map, get_embedding\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4m3s_-LleEk"
      },
      "outputs": [],
      "source": [
        "test_dataset = SentimentDataset(\n",
        "    test_texts, test_labels, glove_model, word_to_vector_map, get_embedding\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jCS2s9aoJW1"
      },
      "source": [
        "# 2a) Report the final configuration of your best model, namely the number of training epochs,learning rate, optimizer, batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe2jX-XI1MrW"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "import RNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LygP_bi1Xd2r"
      },
      "outputs": [],
      "source": [
        "importlib.reload(RNN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX1gKgWsoI3R"
      },
      "outputs": [],
      "source": [
        "from RNN import RNN, train_model_rnn, train_model_multiple_optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY8bZAEmpVzS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train_model_multiple_optimizers(RNN,\n",
        "                               train_loader,\n",
        "                               val_loader,\n",
        "                               num_epochs=40,\n",
        "                               device=device,\n",
        "                               model_name='rnn_model',\n",
        "                               num_runs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpUEqJDcjm-0"
      },
      "outputs": [],
      "source": [
        "import RNN\n",
        "importlib.reload(RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d42-nbXAjwmH"
      },
      "outputs": [],
      "source": [
        "from RNN import RNN, train_model_rnn, train_model_multiple_learning_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sBuQ0v-j4lY"
      },
      "outputs": [],
      "source": [
        "train_model_multiple_learning_rates(RNN,\n",
        "                                    train_loader,\n",
        "                                    val_loader, device,\n",
        "                                    model_name='rnn_model',\n",
        "                                    num_runs=3,\n",
        "                                    num_epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_YcqrRAmN0q"
      },
      "outputs": [],
      "source": [
        "import RNN\n",
        "importlib.reload(RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-MEm4IWmRBy"
      },
      "outputs": [],
      "source": [
        "from RNN import RNN, train_model_rnn, train_model_multiple_batch_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMKZ8Mk2lW9s"
      },
      "outputs": [],
      "source": [
        "train_model_multiple_batch_sizes(RNN,\n",
        "                                 train_data = train_dataset,\n",
        "                                 val_data = val_dataset,\n",
        "                                 device = device,\n",
        "                                 model_name = 'rnn_model',\n",
        "                                 num_runs=3,\n",
        "                                 num_epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)"
      ],
      "metadata": {
        "id": "Q6siwYfms1Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import RNN\n",
        "importlib.reload(RNN)"
      ],
      "metadata": {
        "id": "GSkt7aK7zaF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from RNN import RNN, RNNWithPooling, train_model_rnn"
      ],
      "metadata": {
        "id": "haS0CROfzfhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2b) Report the accuracy score on the test set, as well as the accuracy score on the validation set for each epoch during training."
      ],
      "metadata": {
        "id": "mFAcF9DuGbYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN()\n",
        "\n",
        "# Train model with max pooling, learning rate 0.005, and Adam optimizer\n",
        "history, early_stop_epoch, early_stop_history = train_model_rnn(\n",
        "    model, train_loader, val_loader, num_epochs=40, device=device, model_name='rnn_with_optimised_values', learning_rate=0.005, optimizer_type='Adam'\n",
        ")\n",
        "\n",
        "print(early_stop_history)"
      ],
      "metadata": {
        "id": "Nc1Z1Uon4h55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "# Load the best model checkpoint for testing\n",
        "checkpoint_path = 'test_rnn_with_optimised_values.pth'\n",
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "# Assuming test_loader is defined and contains the test dataset\n",
        "with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass to get predictions\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)  # Get the class with highest probability\n",
        "\n",
        "        # Update metrics\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Print test_correct and test_total\n",
        "print(f'Correct Predictions: {test_correct}')\n",
        "print(f'Total Examples: {test_total}')\n",
        "\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = 100.0 * test_correct / test_total\n",
        "print()\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "# Print early stopping history\n",
        "print(early_stop_history)\n"
      ],
      "metadata": {
        "id": "64XSWE_pjE6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_max = RNNWithPooling(pooling_type='max')\n",
        "\n",
        "# Train model with max pooling, learning rate 0.005, and Adam optimizer\n",
        "history_max_pooling, early_stop_epoch_max_pooling, early_stop_history_max_pooling = train_model_rnn(\n",
        "    model, train_loader, val_loader, num_epochs=40, device=device, model_name='rnn_with_max_pooling', learning_rate=0.005, optimizer_type='Adam'\n",
        ")\n",
        "\n",
        "print(early_stop_history_max_pooling)"
      ],
      "metadata": {
        "id": "DEE92jd_zIdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "# Load the best model checkpoint for testing (with max pooling)\n",
        "checkpoint_path = 'test_rnn_with_max_pooling.pth'\n",
        "model_max.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_max.eval()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "# Assuming test_loader is defined and contains the test dataset\n",
        "with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass to get predictions\n",
        "        outputs = model_max(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)  # Get the class with highest probability\n",
        "\n",
        "        # Update metrics\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Print test_correct and test_total\n",
        "print(f'Correct Predictions: {test_correct}')\n",
        "print(f'Total Examples: {test_total}')\n",
        "\n",
        "# Calculate and print test accuracy\n",
        "test_accuracy = 100.0 * test_correct / test_total\n",
        "print()\n",
        "print(f'Test Accuracy for Max Pooling: {test_accuracy:.2f}%')\n",
        "\n",
        "# Print early stopping history\n",
        "print(early_stop_history_max_pooling)"
      ],
      "metadata": {
        "id": "gYqkNG4YkHFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_avg = RNNWithPooling(pooling_type='avg')\n",
        "\n",
        "# Train model with avg pooling, learning rate 0.005, and Adam optimizer\n",
        "history_avg_pooling, early_stop_epoch_avg_pooling, early_stop_history_avg_pooling = train_model_rnn(\n",
        "    model, train_loader, val_loader, num_epochs=40, device=device, model_name='rnn_with_avg_pooling', learning_rate=0.005, optimizer_type='Adam'\n",
        ")\n",
        "\n",
        "print(early_stop_history_avg_pooling)"
      ],
      "metadata": {
        "id": "WJDglVlJ2iFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "# Load the best model checkpoint for testing (with avg pooling)\n",
        "checkpoint_path = 'test_rnn_with_avg_pooling.pth'\n",
        "model_avg.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_avg.eval()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "# Assuming test_loader is defined and contains the test dataset\n",
        "with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass to get predictions\n",
        "        outputs = model_avg(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)  # Get the class with highest probability\n",
        "\n",
        "        # Update metrics\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Print test_correct and test_total\n",
        "print(f'Correct Predictions: {test_correct}')\n",
        "print(f'Total Examples: {test_total}')\n",
        "\n",
        "# Calculate and print test accuracy\n",
        "test_accuracy = 100.0 * test_correct / test_total\n",
        "print()\n",
        "print(f'Test Accuracy for Avg Pooling: {test_accuracy:.2f}%')\n",
        "\n",
        "# Print early stopping history\n",
        "print(early_stop_history_avg_pooling)\n"
      ],
      "metadata": {
        "id": "KT--pqzekeKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2c) RNNs produce a hidden vector for each word, instead of the entire sentence. Which methods have you tried in deriving the final sentence representation to perform sentiment classification? Describe all the strategies you have implemented, together with their accuracy scores on the test set."
      ],
      "metadata": {
        "id": "YBpeXCMJGp2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training accuracy for max pooling, avg pooling, and default RNN\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(early_stop_history_max_pooling['train_acc']) + 1), early_stop_history_max_pooling['train_acc'], label='Max Pooling - Training Accuracy')\n",
        "plt.plot(range(1, len(early_stop_history_avg_pooling['train_acc']) + 1), early_stop_history_avg_pooling['train_acc'], label='Avg Pooling - Training Accuracy')\n",
        "plt.plot(range(1, len(early_stop_history['train_acc']) + 1), early_stop_history['train_acc'], label='Optimized RNN - Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Accuracy (%)')\n",
        "plt.title('Training Accuracy for Max Pooling, Avg Pooling, and Optimized RNN')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot validation accuracy for max pooling, avg pooling, and default RNN\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(early_stop_history_max_pooling['val_acc']) + 1), early_stop_history_max_pooling['val_acc'], label='Max Pooling - Validation Accuracy')\n",
        "plt.plot(range(1, len(early_stop_history_avg_pooling['val_acc']) + 1), early_stop_history_avg_pooling['val_acc'], label='Avg Pooling - Validation Accuracy')\n",
        "plt.plot(range(1, len(early_stop_history['val_acc']) + 1), early_stop_history['val_acc'], label='Optimized RNN - Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.title('Validation Accuracy for Max Pooling, Avg Pooling, and Optimized RNN')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AbJOOend6g9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import RNN\n",
        "importlib.reload(RNN)"
      ],
      "metadata": {
        "id": "nWsydCj379zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from RNN import RNN, RNNWithConcatPooling, train_model_rnn"
      ],
      "metadata": {
        "id": "-hWZmB4b8DSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNWithConcatPooling()\n",
        "\n",
        "# Train model with max pooling, learning rate 0.005, and Adam optimizer\n",
        "history_con, early_stop_epoch_con, early_stop_history_con = train_model_rnn(\n",
        "    model, train_loader, val_loader, num_epochs=40, device=device, model_name='rnn_with_optimised_values', learning_rate=0.005, optimizer_type='Adam'\n",
        ")\n",
        "\n",
        "print(early_stop_history_con)"
      ],
      "metadata": {
        "id": "XF05UIEq8oPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training accuracy for max pooling, avg pooling, and default RNN\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(early_stop_history_max_pooling['train_acc']) + 1), early_stop_history_max_pooling['train_acc'], label='Max Pooling - Training Accuracy')\n",
        "plt.plot(range(1, len(early_stop_history_con['train_acc']) + 1), early_stop_history_con['train_acc'], label='Concat Pooling - Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Accuracy (%)')\n",
        "plt.title('Training Accuracy for Max Pooling vs Concat Pooling')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot validation accuracy for max pooling, avg pooling, and default RNN\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, len(early_stop_history_max_pooling['val_acc']) + 1), early_stop_history_max_pooling['val_acc'], label='Max Pooling - Validation Accuracy')\n",
        "plt.plot(range(1, len(early_stop_history_con['val_acc']) + 1), early_stop_history_con['val_acc'], label='Concat Pooling - Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.title('Validation Accuracy for Max Pooling vs Concat Pooling')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GwnT8y0G-Jsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For sentiment classification, we have used different methods to derive the final sentence representation from the RNN's hidden states, which are produced for each word. Below, we describe each strategy implemented and their corresponding accuracy scores on the test set:\n",
        "\n",
        "1. **Max Pooling**:\n",
        "   - **Approach**: This method takes the maximum value across the hidden states produced by the RNN for each word in the sequence. It extracts the most prominent feature along the time dimension, which represents the strongest response for each feature across all words in the sequence.\n",
        "   - **Implementation**: In the code, `torch.max(out, dim=1)` was used to compute the max-pooled representation of the hidden states.\n",
        "   - **Accuracy**: The test accuracy with max pooling was higher compared to the baseline RNN due to its ability to retain the most important features, which often helps in capturing critical sentiment-related words in the text.\n",
        "\n",
        "2. **Average Pooling**:\n",
        "   - **Approach**: Average pooling computes the mean value of the hidden states across the sequence, providing an overall representation of the entire sentence by averaging the contributions from each word. This approach tends to capture a more generalized view of the sentence by smoothing out extreme values.\n",
        "   - **Implementation**: In the code, `torch.mean(out, dim=1)` was used to compute the average-pooled representation.\n",
        "   - **Accuracy**: The test accuracy with average pooling was comparable to max pooling but slightly lower in some cases, as it can dilute the impact of highly sentiment-bearing words by averaging them with neutral words.\n",
        "\n",
        "3. **Concatenation of Last Hidden State and Max Pooling**:\n",
        "   - **Approach**: This method combines the information from the last hidden state of the RNN, which contains sequential information, with the max-pooled representation of all hidden states. The concatenation aims to provide both the sequential context and the most prominent features in the sentence.\n",
        "   - **Implementation**: In the code, `torch.cat((last_hidden, max_pooling), dim=1)` was used to concatenate the last hidden state with the max-pooled hidden states.\n",
        "   - **Accuracy**: The concatenation method yielded the best test accuracy among the three approaches, as it was able to leverage both the sequential representation (last hidden state) and the important features (max pooling). This combination allowed the model to capture a richer understanding of the sentence.\n",
        "\n",
        "### Summary of Accuracy Scores:\n",
        "- **Max Pooling**: Higher accuracy than the baseline RNN, effectively capturing key features.\n",
        "- **Average Pooling**: Provided a more generalized sentence representation, with slightly lower accuracy compared to max pooling.\n",
        "- **Concatenation of Last Hidden State and Max Pooling**: Achieved the best accuracy due to the combination of sequential and prominent features, making it a robust representation for sentiment analysis.\n",
        "\n",
        "The plots for training and validation accuracy show the comparative performance of these methods, with concatenation consistently outperforming the others in capturing sentiment effectively."
      ],
      "metadata": {
        "id": "fJfuc0cvIxNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs, device, model_name):\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    # Create an embedding layer as a proper nn.Module\n",
        "    class LearnableEmbeddings(nn.Module):\n",
        "        def __init__(self, dim=300):\n",
        "            super().__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "        def forward(self, x):\n",
        "            return x * self.weight\n",
        "\n",
        "    embedding_layer = LearnableEmbeddings().to(device)\n",
        "\n",
        "    # Combine all parameters\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': model.parameters()},\n",
        "        {'params': embedding_layer.parameters()}\n",
        "    ], lr=0.005, weight_decay=0.01)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=5,\n",
        "        T_mult=2,\n",
        "        eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0\n",
        "    patience = 7\n",
        "    patience_counter = 0\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': [],\n",
        "        'lr': []\n",
        "    }\n",
        "\n",
        "    ema = torch.optim.swa_utils.AveragedModel(model)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        embedding_layer.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_inputs, labels in train_loader:\n",
        "            batch_inputs = batch_inputs.float().to(device)\n",
        "            batch_inputs = embedding_layer(batch_inputs)\n",
        "\n",
        "            labels = labels.squeeze().to(device)\n",
        "\n",
        "            if epoch > 3:\n",
        "                alpha = 0.2\n",
        "                lam = np.random.beta(alpha, alpha)\n",
        "                index = torch.randperm(batch_inputs.size(0)).to(device)\n",
        "                mixed_inputs = lam * batch_inputs + (1 - lam) * batch_inputs[index]\n",
        "                batch_inputs = mixed_inputs\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_inputs)\n",
        "\n",
        "            if epoch > 3:\n",
        "                loss = lam * criterion(outputs, labels) + (1 - lam) * criterion(outputs, labels[index])\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                list(model.parameters()) + list(embedding_layer.parameters()),\n",
        "                max_norm=1.0\n",
        "            )\n",
        "            optimizer.step()\n",
        "            ema.update_parameters(model)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_acc = 100. * correct / total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        embedding_layer.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_inputs, labels in val_loader:\n",
        "                batch_inputs = batch_inputs.float().to(device)\n",
        "                batch_inputs = embedding_layer(batch_inputs)\n",
        "\n",
        "                labels = labels.squeeze().to(device)\n",
        "                outputs = ema(batch_inputs)\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "        scheduler.step()\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "        history['train_loss'].append(total_loss/len(train_loader))\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Training Loss: {total_loss/len(train_loader):.4f}')\n",
        "        print(f'Training Accuracy: {train_acc:.2f}%')\n",
        "        print(f'Validation Accuracy: {val_acc:.2f}%')\n",
        "        print(f'Learning Rate: {current_lr:.6f}')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "            print(f'New best validation accuracy! Saving model...')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': ema.state_dict(),\n",
        "                'embedding_state_dict': embedding_layer.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'train_acc': train_acc,\n",
        "                'history': history,\n",
        "            }, f'best_{model_name}.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping triggered after epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    return best_model_state, best_val_acc, embedding_layer\n"
      ],
      "metadata": {
        "id": "UEqfDcUpsHUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import RNN\n",
        "importlib.reload(RNN)\n"
      ],
      "metadata": {
        "id": "t41T-9JEt1p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from RNN import RNNWithConcatPooling"
      ],
      "metadata": {
        "id": "TeLWNdqguCud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3A Accuracy Score with Dynamic Embeddings\n"
      ],
      "metadata": {
        "id": "ls2buCCKyixi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RNNWithdynamicembeddings = RNNWithConcatPooling()\n",
        "\n",
        "RNNmodelstate, _,_ = train_model(RNNWithdynamicembeddings, train_loader, val_loader, num_epochs=30, device=device, model_name='RNNWithdynamicembeddings')\n",
        "RNNWithdynamicembeddings.load_state_dict(RNNmodelstate)"
      ],
      "metadata": {
        "id": "o_m1mcK7uI6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy Score with OOV Handling"
      ],
      "metadata": {
        "id": "M8KyhRl_ywAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.linalg import lstsq\n",
        "def get_improved_embedding(word, models, word_to_vector_map):\n",
        "    \"\"\"\n",
        "    Retrieve the embedding for a word from GloVe, or from FastText (transformed to GloVe space),\n",
        "    or generate a random embedding if OOV in both, storing it in oov_vectors if new.\n",
        "    \"\"\"\n",
        "    glove_model = models[\"glove\"]\n",
        "    fasttext_model = models[\"fasttext\"]\n",
        "    if word in word_to_vector_map:\n",
        "        return word_to_vector_map[word]\n",
        "    if word in glove_model:\n",
        "        return glove_model[word]\n",
        "\n",
        "    # Compute the FastText-to-GloVe transformation matrix only once per session\n",
        "    if not hasattr(get_improved_embedding, \"W_fasttext\"):\n",
        "        # Find common words in both FastText and GloVe models\n",
        "        common_words = list(set(glove_model.key_to_index).intersection(set(fasttext_model.key_to_index)))\n",
        "        X_fasttext = np.array([fasttext_model[w] for w in common_words])\n",
        "        Y_glove = np.array([glove_model[w] for w in common_words])\n",
        "        get_improved_embedding.W_fasttext, _, _, _ = lstsq(X_fasttext, Y_glove)\n",
        "\n",
        "\n",
        "    # If the word is in FastText, transform its embedding to GloVe space\n",
        "    if word in fasttext_model:\n",
        "        transformed_embedding = np.dot(fasttext_model[word], get_improved_embedding.W_fasttext)\n",
        "        return transformed_embedding\n",
        "    word_to_vector_map[word] = np.random.normal(size=300)\n",
        "    return word_to_vector_map[word]\n",
        "train_dataset = SentimentDataset(\n",
        "    train_texts, train_labels, glove_model, word_to_vector_map, get_improved_embedding\n",
        ")\n",
        "\n",
        "val_dataset = SentimentDataset(\n",
        "    val_texts, val_labels, glove_model, word_to_vector_map, get_improved_embedding\n",
        ")\n"
      ],
      "metadata": {
        "id": "VbOJKVMhusBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RNNWithoovhandling = RNNWithConcatPooling()\n",
        "\n",
        "RNNmodelstate, _,_ = train_model(RNNWithoovhandling, train_loader, val_loader, num_epochs=30, device=device, model_name='RNNWithoovhandling')\n",
        "RNNWithoovhandling.load_state_dict(RNNmodelstate)"
      ],
      "metadata": {
        "id": "dqNXemxFyNhA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}